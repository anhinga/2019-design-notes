\documentclass{article}

\usepackage{hyperref}
\usepackage{tikz}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{endnotes}

\definecolor{myblue}{rgb}{0, 0, 0.9}

\newcommand{\msblue}[1]{{\color{myblue} #1}}


\title{Dataflow matrix machines:\\ - duality between {\bf W} and its input;
                                              \\ - shader-style non-linear component}
\author{Michael Bukatin}

\begin{document}

\maketitle

\begin{abstract}
We look at the intriguing natural duality between {\bf W} and {\bf Y}. This might eventually result in novel
self-referential mechanisms in DMMs. 

We also note that thinking about
the non-linear part of the network as a shader might be quite fruitful. 

It was observed before that compositional pattern-producing networks have shader architecture.


\end{abstract}

\section*{Introduction}

This research note emerged as a side effect of our design efforts and preliminary code explorations for the next generation of DMMs\endnote{For the state of DMMs as of November 2018, see {\bf DMM technical report 11-2018}, "Dataflow matrix machines:
recent experiments and notes for next steps", \url{https://github.com/jsa-aerial/DMM/tree/master/technical-report-2018}. 

For the design efforts for the next generation of DMMs, see \url{https://github.com/anhinga/2019-design-notes}. 
For related preliminary code explorations, see \url{https://github.com/anhinga/2019-python-drafts}.}. One particular focus of this new effort is that one would like to retain the full flexibility of DMMs, but at the same time to be able to efficiently train and run them on GPUs.

\section{Duality in {\bf W}$\cdot${\bf Y}}

We consider a standard two-stroke cycle neural machine with vector flows. The two-stroke cycle has a linear part controlled by matrix {\bf W} and a non-linear part (traditionally understood as "activation functions of neurons do the work"). 

\smallskip

We denote the non-linear part as {\bf F}, so the overall cycle of the machine is {\bf F}$\circ${\bf W} (or one might prefer {\bf W}$\circ${\bf F}, but we have found it more convenient to apply {\bf W} first, so our standard view is {\bf F}$\circ${\bf W}; both views are quite useful).

\bigskip

Consider the flavor where the vectors in question have a fixed finite dimension and are shaped to form one-dimensional arrays.

\smallskip

The linear part applies {\bf W} to the vector of vectors (data produced by the action of the non-linear part). This vector of vectors forms matrix {\bf Y}. The application of {\bf W} to this vector of vectors is simply matrix multiplication, {\bf W}$\cdot${\bf Y}.

\bigskip

The obvious duality here is that there is no particular way to distinguish between {\bf W} and {\bf Y}, especially if both are
generated by the action of the non-linear part, as typical for DMMs. If one wants to preserve the convention that the network matrix is on the left, all one needs to do for the duality is to transpose the whole thing: {\bf X}$^{\top}$ = {\bf Y}$^{\top}\cdot${\bf W}$^{\top}$.

\section{Rectangular {\bf W} vs. square {\bf W}}


There is tension in the field between the desire to view $\bf W$ as a square matrix and the desire to view it as a rectangular matrix.

\bigskip

$\bf W$ as a square matrix brings with itself all wonderfully convenient things,
like eigenvalues, and to study the resulting dynamic systems by simply expecting $\bf F$ to have properties good enough to not interfere much compared to the case {\bf F} = {\bf Id} (the case when {\bf F} can be completely ignored).

\smallskip

E.g. a nice recent study introducing {\em recurrent identity networks}\endnote{See "Overcoming the vanishing gradient problem in plain recurrent networks" by Yuhuang Hu, Adrian Huber, Jithendar Anumula, Shih-Chii Liu, \url{https://arxiv.org/abs/1801.06105}; see also my note "Understanding Recurrent Identity Networks", \url{https://www.cs.brandeis.edu/~bukatin/recurrent-identity-networks.html} written about {\bf v1} of that preprint, \url{https://arxiv.org/abs/1801.06105v1}.} represents {\bf W} as {\bf U}+{\bf Id}, and then when one applies traditional
regularization methods to elements of {\bf U} instead of elements of {\bf W}, the problem of vanishing gradients mostly disappears. (If one would like to progress from empirical observations to mathematical theorems, one would still need to consider the {\bf F}$\circ${\bf W} or {\bf W}$\circ${\bf F} and to explicitly formulate some constraints on the properties of {\bf F} needed for this consideration to hold.)

\bigskip

In this study we'll think about {\bf W} as a rectangular matrix (even when $M$ and $N$ might accidentally coincide).

\smallskip

This will make it easier to emphasize asymmetries between inputs and outputs (both of {\bf W} and of {\bf F}).

\section{Redefining {\bf F} and the two-stroke cycle}

Let's say that {\bf W} is $M\!\!\times\!\!N$ matrix, and {\bf Y} is $N\!\!\times\!\!D$ matrix. 

\smallskip

Here $D$ is the dimensionality of the vectors in our flows, $N$ is the number of vectors produced by the non-linear part, and $M$ is the number of vectors used as the input to the non-linear part. That is, the input to the non-linear part {\bf F} is the 
$M\!\!\times\!\!D$-dimensional matrix {\bf X} = {\bf W}$\cdot${\bf Y}.

\bigskip

It is time to stop thinking about the two-stroke cycle as having a linear part {\bf W}: {\bf Y} $\mapsto$ {\bf X} = {\bf W} $\cdot$ {\bf Y}, and to replace it with a bilinear part {\bf B}: $\langle\!\!$ {\bf W}, {\bf Y} $\!\!\rangle \mapsto$ {\bf X}.

\smallskip

Then, instead of {\bf F}: {\bf X} $\mapsto$ {\bf Y}, we are going to have {\bf F}: {\bf X} $\mapsto \langle\!\!$ {\bf W}, {\bf Y} $\!\!\rangle$.

\smallskip

Under this new way of thinking, the canonical view of the two-stroke cycle, {\bf F}$\circ${\bf B}, maps $\mathbb{R}^{MN+ND}$ to itself, and the alternative view, {\bf B}$\circ${\bf F}, maps $\mathbb{R}^{MD}$ to itself.

\section{Self-referential mechanisms in the past and in the future}

We used the following standard self-referential schema in finite-dimensional DMMs. We required that $MN \!\leq\! D$ (this
condition was unnecessary in the countably-dimensional case), and we used a row of {\bf Y} to contain {\bf W} to be used as
the network matrix on the next linear step of the two-stroke cycle.

\smallskip

Now, as {\bf F} produces not just {\bf Y}, but the pair $\langle\!\!$ {\bf W}, {\bf Y} $\!\!\rangle$, we anticipate that
a more considerable variety of self-referential mechanism becomes available eventually. Embedding of {\bf W} into {\bf Y}, which has been necessary for self-modification, is now optional.

\section{Asymmetries between inputs and outputs}

Considering these asymmetries will help us to generalize from the traditional view of {\bf F} as being assembled from separate neurons.

\smallskip

We also hope that these asymmetries will help us to achieve a better way of handling our attempts to 
consider general {\em V-values} as network matrices\endnote{Section 2.3 of {\bf DMM technical report 11-2018}, "Dataflow matrix machines:
recent experiments and notes for next steps", \url{https://github.com/jsa-aerial/DMM/tree/master/technical-report-2018}. A particularly difficult part was trying to have an arbitrary nested structure of the indices of matrix rows, and to add the resulting outputs together. The present paper seems to be saying that it was misguided to try to do that; instead we should just have arbitrary nested structure of the indices of matrix columns (I am still thinking in terms of {\bf F}$\circ${\bf W} architecture here rather than {\bf F}$\circ${\bf B}; what is really needed is to rethink this from the {\bf F}$\circ${\bf B} viewpoint).}.

\bigskip

In particular, a single row of {\bf W} only produces one row of {\bf X}, but is using the whole {\bf Y} as its input.

\smallskip

If we take a very traditional view of {\bf F}, then it would consist of single neurons mapping one row of {\bf X} into one row of {\bf Y}. A bit more general (but still traditional for DMMs) consideration allows to associate with each neuron its own set of rows of {\bf X} and its own set of rows {\bf Y}, and all these sets are non-intersecting (inputs and outputs are not shared between different neurons). 

\bigskip

Emphasizing the asymmetry between inputs and outputs, the neurons of {\bf F} (if we still insist of thinking about {\bf F}
as stratified into the neurons) should not be weaker than single rows of {\bf W} in any respect. So we can allow them to have
access to all {\bf X} (this thought occurred to us during our earlier experiments with pure lightweight DMMs, 
but never made it into implementation).

\smallskip

At the same time, an element of {\bf F} output (perhaps, a row of {\bf Y} - let's think this way for simplicity for a second) can
be generated by no more than one neuron in {\bf F}. If something in {\bf Y} is not covered by a neuron, we can always add a placeholder const neuron to denote that this part of {\bf Y} does not change.

\bigskip

The generalization we are about to consider is that it is not necessary to stratify the output of {\bf F} by rows (by neurons generating specific rows). 

\smallskip

One still might need to stratify parts of the output of {\bf F} by the method of change. E.g. we've seen earlier that when the network aggressively changes the whole {\bf W} on each step, it is not so easy to handle training signals or manual update signals for {\bf W} (such signals can be received and incorporated into $\Delta${\bf W}, but {\bf W} itself is very much a moving target). 

\section{Shader style for {\bf F}}

We need to produce a pair of matrices $\langle\!\!$ {\bf W}, {\bf Y} $\!\!\rangle$ (or one can think about this as a single matrix $N\times (M+D)$, if more convenient). The natural style here might be shader-like style of {\bf F}, where the resulting real number (or a small-dimensional vector) is a function of coordinates in the output matrix.

\smallskip

This style tends to works nicely on graphic cards (which is why this is essentially the way {\em fragment shaders} (also known as {\em pixel shaders}) work). So this fits well with our goals of comfortably implementing DMMs on GPUs.

\smallskip

This style is often used in generative computer art\endnote{See e.g. "Random art in Python" by Andrej Bauer, \url{http://math.andrej.com/2010/04/21/random-art-in-python/} or "Random Art" by Christopher Stone, \url{http://nifty.stanford.edu/2009/stone-random-art/}}.

\bigskip

It was observed earlier that the input-output structure of compositional pattern-producing networks is the same as the structure of fragment shaders\endnote{For example, see "Interactive CPPNs in GLSL" by Xavier Snelgrove and Matthew Tesfaldet,
\url{https://nips2018creativity.github.io/doc/interactive_cppns_in_glsl.pdf}, who use this to convert CPPNs to GLSL obtaining interactive neural shaders, \url{https://github.com/wxs/cppn-to-glsl}}. This is quite suggestive for possible methods of DMM synthesis.

\bigskip

I am not sure how well this would work for sparse situation, which is typical for DMMs. Sparse textures do exist (e.g. in OpenGL the sparse textures extension exists since version 4.4 (2013)). But at the first glance, the patterns of their use are quite different from the patterns we need (which is, only compute the shader for a selected subset of target points). It should be
not too difficult to express such patterns via multiplicative masks, or even with {\tt if} statements, but would the compiler optimize the results well?

\theendnotes

%\tableofcontents


\end{document}